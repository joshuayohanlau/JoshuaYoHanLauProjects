# -*- coding: utf-8 -*-
"""Logistic Regression Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yNs2sfk_MkT3ac-6R0x_E3MfxxvJvai3
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/churn_cleaned.csv')

df.head()

df.tail()

# I want to encode categorical colums that dont have numbers in them or groups like the ones
#in age group
df = pd.get_dummies(df, columns=['Geography', 'Gender', 'customer_age_group'], drop_first=True)

# im just going to drop the name column as it doesnt do anything to train the model later on
#error is because i ran it twice
df = df.drop(columns=['Surname'])

df.head()

# so i forgot to hot encode the balance category so ill do that now
df = pd.get_dummies(df, columns=['balance_category'], drop_first=True)

df.head()

# so now im going to split the features into x and y
#x being the features used to predict churn, so everything but churn
#and y is just churn

X = df.drop('Exited', axis=1)
y = df['Exited']

# so im going to do a test/train split
# 30% of the data is for testing the rest for training
# the random state and stratify is to ensure results arent skewed
#and also is reproducable

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# now im going to scale it since the number difference for things like balance
# can be quite large

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# now all the preproccessing is done, now we train the model
# so i went with the logistic regression model because it predicts the probability
# of the relationship between churn and the rest of the variables
# i set the max interation to 1000 as it is a larger dataset

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# now i just set the predicted class and then find the probability for each class
#the [:, 1] is because we only want the probabilty of churn, as it spits out
# 2 columns one being probability of not churn which we dont want

y_pred = log_reg.predict(X_test_scaled)
y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]

# now that the model is trained its time to identify key churn drivers
#essentially what i did here is grab the coefficients and then combine feature
# name to the said coefficients

importance = log_reg.coef_[0]

feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': importance
}).sort_values(by='Coefficient', ascending=False)

feature_importance

# just from looking at this we can see that age comes into play again
# as a positive coefficent is a increase in churn risk and a lower is a decrease
# so a higher age increases churn risk, which is exactly what we hypothesised before

# i think it is also useful to try visualise it

plt.figure(figsize=(8,6))
sns.barplot(
    y='Feature',
    x='Coefficient',
    data=feature_importance.sort_values('Coefficient', key=abs, ascending=False).head(10),
    palette='coolwarm'
)
plt.title('Top 10 Features Impacting Customer Churn')
plt.xlabel('Coefficient Value (Impact on Churn)')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

# so from here on top of age being a big reason for customer churn, we see that there
#is something wrong with germany as well, we mentioned this in our previous analysis as well
# so there obviously must be some product or service issue with the german bank branch

